{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Sad-old-things\" data-toc-modified-id=\"Sad-old-things-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sad old things</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOCAL_PATH = '/Users/jacob/Downloads/hackathon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_file_paths(path, extension='.tif'):\n",
    "    \"\"\"List the file paths for every file ending with `extension` in directory `path`\"\"\"\n",
    "    file_paths = []\n",
    "    \n",
    "    for (dir_path, _, file_names) in os.walk(path):\n",
    "        \n",
    "        relative_path = dir_path.replace(path, '')\n",
    "        file_path = [os.path.join(relative_path, file_name) for file_name in file_names\n",
    "                     if file_name.endswith(extension)]\n",
    "\n",
    "        file_paths.extend(file_path)\n",
    "\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    list(map(lambda x: '/'.join(x.split('/')[:-1]), \n",
    "             list_file_paths(LOCAL_PATH + 'Streamline process/measured/') +\n",
    "             list_file_paths(LOCAL_PATH + 'Search engine/test/')\n",
    "            ))).drop_duplicates().to_csv('folder_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_metadata = pd.read_csv('data/folder_taxa_specimen_tissue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_metadata(path):\n",
    "    d = pd.DataFrame(list_file_paths(LOCAL_PATH + path), columns=['path'])\n",
    "    d['folder'] = d.path.apply(lambda x: '/'.join(x.split('/')[:-1]))\n",
    "    d.path = path + d.path\n",
    "    d = d.merge(folder_metadata, on='folder')\n",
    "    d.drop('folder', 1, inplace=True)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([merge_metadata('Search engine/test/'), merge_metadata('Streamline process/measured/')])\\\n",
    "    .to_csv('data/image_database.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sad old things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_specimen_ids(row):\n",
    "    \"\"\"\n",
    "    The taxa and tissue columns both may have extra copies of the specimen id. We remove these.\n",
    "    \"\"\"\n",
    "    \n",
    "    def replace_specimen_id(s):\n",
    "        return s.replace(specimen_id_space, '').replace(specimen_id_no_space, '')\n",
    "\n",
    "    if pd.notnull(row['specimen_id_taxa']):\n",
    "        row['taxa'] = row['taxa'].replace(row['specimen_id_taxa'], '')\n",
    "\n",
    "    if pd.notnull(row['tissue']) and pd.notnull(row['specimen_id_tissue']):\n",
    "        row['tissue'] = row['tissue'].replace(row['specimen_id_tissue'], '')\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_consensus_specimen_id(row):\n",
    "    \n",
    "    taxa_isnull = bool(row['specimen_id_taxa'])\n",
    "    tissue_isnull = bool(row['specimen_id_tissue'])\n",
    "    \n",
    "    if tissue_isnull and taxa_isnull:\n",
    "        return ''\n",
    "    \n",
    "    elif not tissue_isnull and not taxa_isnull:\n",
    "        if row['specimen_id_taxa'] == row['specimen_id_tissue']:\n",
    "            return row['specimen_id_tissue']\n",
    "        else:\n",
    "            print(row)\n",
    "            return row['specimen_id_tissue'] + '-' + row['specimen_id_taxa']\n",
    "        \n",
    "    elif tissue_isnull:\n",
    "        return row['specimen_id_taxa']\n",
    "        \n",
    "    elif taxa_isnull:\n",
    "        return row['specimen_id_tissue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_image_metadata(local_path, intermediate_path):\n",
    "    \"\"\"\n",
    "    local_path: Path unique to your machine.\n",
    "    intermediate_path: Path  to concatenate to local path to get to the files labeleled with the \n",
    "        taxa (genus/species) and (sometimes) specimen id\n",
    "    \"\"\"\n",
    "    \n",
    "    file_paths = list_file_paths(local_path + intermediate_path)\n",
    "    d = pd.DataFrame(file_paths, columns=[\"path\"])\n",
    "    \n",
    "    d['specimen_id_taxa'] = d.path.str.extract('([A-Z]+ ?\\d+)/', expand=False)\n",
    "    d['specimen_id_tissue'] = d.path.str.extract('/[A-Z]?[a-z ]*([A-Z]+ ?\\d*)/', expand=False)\n",
    "    d['taxa'] = d.path.str.extract('(.*?)[_!@]?/', expand=False)\n",
    "    d['tissue'] = d.path.str.extract('/(.*)/', expand=False)\n",
    "    \n",
    "    # after regexing on the more standard paths, prepend the intermediate path\n",
    "#     d.path = intermediate_path + d.path\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(path):\n",
    "    d = extract_image_metadata(LOCAL_PATH, path)\n",
    "    d = d.apply(remove_specimen_ids, axis=1)\n",
    "    \n",
    "    # After using them to clean up the taxa and tissue, normalize the specimen_id_taxa and \n",
    "    # specimen_id_tissue\n",
    "    d['specimen_id_taxa'] = d.specimen_id_taxa.fillna('').str.replace(' ', '')\n",
    "    d['specimen_id_tissue'] = d.specimen_id_tissue.fillna('').str.replace(' ', '')\n",
    "    \n",
    "    d['specimen_id'] = d.apply(find_consensus_specimen_id, axis=1)\n",
    "    \n",
    "    d.taxa = d.taxa.str.lower().str.replace('_', '')\n",
    "    d.tissue = d.tissue.str.lower().str.strip()\n",
    "    return d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
